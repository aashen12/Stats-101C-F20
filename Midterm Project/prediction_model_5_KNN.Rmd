---
title: 'Model #5: KNN'
author: 'Ethan Allavarpu (UID: 405287603)'
date: "10/28/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Transforming and Cleaning the Data
```{r transform_clean}
training <- read.csv("training.csv", stringsAsFactors = TRUE)
training$class <- factor(training$class)
levels(training$class) <- c("NG", "OG", "TSG")
outlier <- function(data) {
  low <- mean(data) - 3 * sd(data)
  high <- mean(data) + 3 * sd(data)
  which(data < low | data > high)
}
library(ggplot2)
scatter <- function(var) {
  ggplot(training, aes_string(var, "class")) +
    geom_jitter(width = 0.05, height = 0.1, size = 0.1,
                colour = rgb(0, 0, 0, alpha = 1 / 3))
}
scat_plot <- lapply(names(training)[-99], scatter)
library(gridExtra)
# grid.arrange(grobs = scat_plot[1:20], ncol = 4)
# grid.arrange(grobs = scat_plot[21:40], ncol = 4)
# grid.arrange(grobs = scat_plot[41:60], ncol = 4)
# grid.arrange(grobs = scat_plot[61:80], ncol = 4)
# grid.arrange(grobs = scat_plot[81:98], ncol = 4)
outlier_index <- sort(table(unlist(lapply(training[,-99], outlier))), decreasing = TRUE)
outlier_index[1:100]
training <- training[-as.numeric(names(outlier_index)[1:50]),]
training <- training[-which(training$Missense_TO_Silent_Ratio > 100), ]
training <- training[-which(training$Missense_KB_Ratio > 2000), ]
training <- training[-which(training$LOF_TO_Silent_Ratio > 5), ]
training <- training[-which(training$Gene_expression_Z_score > 4), ]
training <- training[-which(training$dN_to_dS_ratio > 5),]
training <- training[-which(training$Silent_KB_Ratio > 200), ]
training <- training[-which(training$Lost_start_and_stop_fraction > 0.2),]
```

```{r}
library(ggplot2)
scatter <- function(var) {
  ggplot(training, aes_string(var, "class")) +
    geom_jitter(width = 0.05, height = 0.1, size = 0.1,
                colour = rgb(0, 0, 0, alpha = 1 / 3))
}
scat_plot <- lapply(names(training)[-99], scatter)
library(gridExtra)
grid.arrange(grobs = scat_plot[1:20], ncol = 4)
grid.arrange(grobs = scat_plot[21:40], ncol = 4)
grid.arrange(grobs = scat_plot[41:60], ncol = 4)
grid.arrange(grobs = scat_plot[61:80], ncol = 4)
grid.arrange(grobs = scat_plot[81:98], ncol = 4)
```

```{r}
library(dplyr)
sig <- logical(98)
names(sig) <- names(training)[-99]
k <- 1
diffs <- logical(98)
for (var in names(training)[-99]) {
  model <- aov(training[[var]] ~ factor(training$class))
  sig[k] <- summary(model)[[1]][1, 5]
  diffs[k] <- all(TukeyHSD(model)$`factor(training$class)`[, 4] < 0.05)
  k <- k + 1
}
sort(sig[diffs])
score <- function (conf_mat) {
  print(sum(diag(conf_mat) * c(1, 20, 20)))
  print(sum(diag(conf_mat) * c(1, 20, 20)) / sum(apply(conf_mat, 2, sum) * c(1, 20, 20)))
}

classify <- function(probs) {
  if (any(probs[2:3] > 1 / 41)) {
    subset <- probs[2:3]
    output <- which(subset == max(subset))
    if (length(output) > 1) {
        output <- sample(1:2, 1)
    }
  } else {
    output <- 0
  }
  output
}
```



```{r, fig.height = 7, fig.width = 7}
library(dplyr)
vars <- training %>% select(Broad_H3K9ac_percentage, N_LOF, pLOF_Zscore,
                             Missense_Entropy,
                            N_Splice, LOF_TO_Total_Ratio, VEST_score,
                            BioGRID_log_degree,
                            Broad_H3K79me2_percentage, FamilyMemberCount,
                            S50_score_replication_timing, Gene_expression_Z_score,
                            Polyphen2, Broad_H3K36me3_percentage, class)
vars$class <- factor(vars$class)
levels(vars$class) <- c("NG", "OG", "TSG")
cor_mtx = round(cor(vars[, names(vars) != "class"]), 2)
library(reshape2)
#reshape it
melted_cor_mtx <- melt(cor_mtx)

#draw the heatmap
cor_heatmap = ggplot(data = melted_cor_mtx, aes(x=Var1, y=Var2, fill=value)) + geom_tile()
cor_heatmap = cor_heatmap +
scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1,1), space = "Lab", name="Pearson\nCorrelation") + 
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1))

cor_heatmap
```

```{r}
library(dplyr)
distances <- function(data) {
  dist_mat <- matrix(nrow = length(data), ncol = length(data))
  for (i in seq_len(nrow(dist_mat))) {
    for (j in seq_len(ncol(dist_mat))) {
      dist_mat[i, j] <- sqrt((data[i] - data[j])^2) 
    }
  }
  total_avg <- mean(dist_mat)
  subsets <- list()
  for (k in levels(training$class)) {
    subsets[[i]] <- dist_mat[training$class == k, training$class == k]
  }
  subsets <- unlist(subsets)
  mean(subsets) / total_avg
}
get_pairwise_dists <- function(m)
{
  num_rows = nrow(m)
  num_cols = ncol(m)
  
  m_1 = matrix(rep(as.matrix(m), each=num_rows), c(num_rows^2, num_cols), byrow=FALSE)
  m_2 = matrix(rep(t(as.matrix(m)), num_rows), c(num_rows^2, num_cols), byrow=TRUE)
  dists = sqrt(rowSums((m_1 - m_2)^2))
  D = matrix(dists, c(num_rows,num_rows), byrow=TRUE)
  return(D)
}

```


```{r}
process_bootstrap_samples <- function(d, num_bootstrap_samples)
{
  var_names = names(d)[names(d) != 'class']
  r_vals = rep(0, num_bootstrap_samples)
  for (i in 1:num_bootstrap_samples)
  {
    #generate a random bootstrap sample (replace=TRUE is CRUCIAL)
    bootstrap_sample = d[sample(nrow(d), nrow(d), replace=TRUE), ]
    
    #compuate the within-class pairwise distances
    c0_dists = get_pairwise_dists(bootstrap_sample[bootstrap_sample$class == 0, var_names])
    c1_dists = get_pairwise_dists(bootstrap_sample[bootstrap_sample$class == 1, var_names])
    c2_dists = get_pairwise_dists(bootstrap_sample[bootstrap_sample$class == 2, var_names])
    
    #compuate all pairwise distances
    all_dists = get_pairwise_dists(bootstrap_sample[, var_names])
    
    #compuate the clustering coefficient 
    r = mean(c(c0_dists, c1_dists, c2_dists)) / mean(all_dists)
    r_vals[i] = r
  }
  
  hist(r_vals, main=var_names)
  abline(v=mean(r_vals))
}

set.seed(123)
vars <- training %>% select(Broad_H3K9ac_percentage, N_LOF, pLOF_Zscore,
                             Missense_Entropy,
                            N_Splice, LOF_TO_Total_Ratio, VEST_score,
                            BioGRID_log_degree,
                            Broad_H3K79me2_percentage, FamilyMemberCount,
                            S50_score_replication_timing, Gene_expression_Z_score,
                            Polyphen2, Broad_H3K36me3_percentage, class)

library(caret)
vars_test <- createDataPartition(vars$class, p = 0.8, 
                                  list = FALSE)
vars_train <- vars[vars_test, ]
vars_test <- vars[-vars_test, ]

train_cont <- trainControl(method = "cv", number = 5, classProbs = TRUE, savePredictions = TRUE)


knn_ft <- train(class ~ ., data = vars_train, method = "knn", preProc = c("center", "scale"),
                trControl = train_cont, tuneGrid = expand.grid(k = seq(from = 1, to = 25, by = 5)))
for (k in seq(from = 1, to = 25, by = 5)) {
  preds <- predict(knn_ft, newdata = vars_test, type = "prob")
  knn_mod <- table("pred"=unlist(apply(preds, 1, classify)), "obs" = vars_test$class)
  print(knn_mod)
  score(knn_mod)
}
```

```{r create_preds}
tests <- read.csv("test.csv")
preds <- predict(lda_ft, newdata = tests, type = "prob")
preds <- apply(preds, 1, classify)
names(preds) <- tests$id
csv_file <- data.frame("id" = tests$id,
                       "class" = preds)
# write.csv(csv_file, "modelpredictions4.csv", row.names = FALSE)
model_1 <- read.csv("modelpredictions.csv")
model_2 <- read.csv("modelpredictions2.csv")
model_1_beta <- read.csv("modelpredictions_beta.csv")
model_2_beta <- read.csv("modelpredictions2_beta.csv")
model_3 <- read.csv("modelpredictions3.csv")
table(model_1$class, model_1_beta$class)
table("BEST MOD" = model_3$class, csv_file$class)
mean(model_1$class == csv_file$class)
```