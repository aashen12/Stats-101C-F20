---
title: "Model #6"
author: 'Ethan Allavarpu (UID: 405287603)'
date: "10/28/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA)
```

## Transforming and Cleaning the Data
```{r transform_clean}
training <- read.csv("training.csv", stringsAsFactors = TRUE)
training$class <- factor(training$class)
levels(training$class) <- c("NG", "OG", "TSG")
outlier <- function(data) {
  low <- mean(data) - 3 * sd(data)
  high <- mean(data) + 3 * sd(data)
  which(data < low | data > high)
}
library(ggplot2)
scatter <- function(var) {
  ggplot(training, aes_string(var, "class")) +
    geom_jitter(width = 0.05, height = 0.1, size = 0.1,
                colour = rgb(0, 0, 0, alpha = 1 / 3))
}
scat_plot <- lapply(names(training)[-99], scatter)
library(gridExtra)
# grid.arrange(grobs = scat_plot[1:20], ncol = 4)
# grid.arrange(grobs = scat_plot[21:40], ncol = 4)
# grid.arrange(grobs = scat_plot[41:60], ncol = 4)
# grid.arrange(grobs = scat_plot[61:80], ncol = 4)
# grid.arrange(grobs = scat_plot[81:98], ncol = 4)
outlier_index <- sort(table(unlist(lapply(training[,-99], outlier))), decreasing = TRUE)
outlier_index[1:100]
training <- training[-as.numeric(names(outlier_index)[1:50]),]
sort(training$Missense_TO_Silent_Ratio, decreasing = TRUE)[1:10]
training <- training[-which(training$Missense_TO_Silent_Ratio > 100), ]
sort(training$Missense_KB_Ratio, decreasing = TRUE)[1:10]
training <- training[-which(training$Missense_KB_Ratio > 2000), ]
sort(training$LOF_TO_Silent_Ratio, decreasing = TRUE)[1:10]
training <- training[-which(training$LOF_TO_Silent_Ratio > 5), ]
sort(training$Gene_expression_Z_score, decreasing = TRUE)[1:10]
training <- training[-which(training$Gene_expression_Z_score > 4), ]
sort(training$dN_to_dS_ratio, decreasing = TRUE)[1:10]
training <- training[-which(training$dN_to_dS_ratio > 5),]
sort(training$Silent_KB_Ratio, decreasing = TRUE)[1:10]
training <- training[-which(training$Silent_KB_Ratio > 200), ]
sort(training$Lost_start_and_stop_fraction, decreasing = TRUE)[1:10]
training <- training[-which(training$Lost_start_and_stop_fraction > 0.2),]
sort(training$Synonymous_Zscore, decreasing = FALSE)[1:10]
training <- training[-which(training$Synonymous_Zscore < -15), ]
numeric_training <- training[,-99]

n_zeroes <- rep(NA, nrow(numeric_training))

for(i in seq_len(nrow(numeric_training))){
  row_i_zeroes <- 0
  for(j in seq_len(ncol(numeric_training))){
    if(round(numeric_training[i,j], digits = 5) == 0){
      row_i_zeroes <- row_i_zeroes + 1
    }
  }
  n_zeroes[i] <- row_i_zeroes
}
training <- training[n_zeroes <= 50, ]
```


```{r}
library(dplyr)
#function to calculate wca
score <- function (conf_mat) {
  print(sum(diag(conf_mat) * c(1, 20, 20)))
  print(sum(diag(conf_mat) * c(1, 20, 20)) / sum(apply(conf_mat, 2, sum) * c(1, 20, 20)))
}

# Set new threshold to account for unbalanced data
classify <- function(probs) {
  if (any(probs[2:3] > 0.05)) {
    subset <- probs[2:3]
    output <- which(subset == max(subset))
    if (length(output) > 1) {
        output <- sample(1:2, 1)
    }
  } else {
    output <- 0
  }
  output
}
```

\pagebreak

## Multinom (Logistic Regression)

```{r}
library(dplyr)
library(caret)
set.seed(43)
# vars <- training %>% select(Broad_H3K9ac_percentage, N_LOF, pLOF_Zscore,
#                              Missense_Entropy,
#                             N_Splice, LOF_TO_Total_Ratio, VEST_score,
#                             BioGRID_log_degree,
#                             Broad_H3K79me2_percentage, FamilyMemberCount,
#                             S50_score_replication_timing, Gene_expression_Z_score,
#                             Polyphen2, Broad_H3K36me3_percentage, class)

vars_index <- createDataPartition(training$class, p = 0.76, 
                                  list = FALSE)
vars_train <- training[vars_index, ] #was formerly vars[vars_index,]...
vars_test <- training[-vars_index, ]

mn <- nnet::multinom(class ~ ., data = vars_train, model = TRUE)
tidymn <- broom::tidy(mn) %>% arrange(p.value)
terms <- tidymn$term[-(1:2)]
terms_unique <- unique(terms)
top_14 <- terms_unique[1:14]
cat(top_14, sep = ",")
```


```{r}
#set.seed(9) gives 0.82
#set.seed(2) gives 0.77
#set.seed(12) 0.81
#set.seed(3275) gives 0.76
#set.seed(999) gives 0.77
#set.seed(1235) gives 0.84
#set.seed(712) gives 0.79
#set.seed(100) gives 0.799
#set.seed(200) gives 0.71
set.seed(712)
vars_mn <- training %>% select(
  LOF_TO_Silent_Ratio,Splice_TO_Silent_Ratio,Missense_TO_Silent_Ratio,LOF_TO_Benign_Ratio,Splice_TO_Benign_Ratio,Missense_Damaging_TO_Benign_Ratio,Polyphen2,LOF_TO_Total_Ratio,Missense_TO_Total_Ratio,Splice_TO_Total_Ratio,LOF_TO_Missense_Ratio,Silent_fraction,Nonsense_fraction,Missense_fraction, class
)

vars_index <- createDataPartition(vars_mn$class, p = 0.76, 
                                  list = FALSE)
vars_train <- training[vars_index, ] # it's being overridden
vars_test <- training[-vars_index, ]

mn <- nnet::multinom(class ~ ., data = vars_train, model = TRUE)

tests <- read.csv("test.csv")
preds <- predict(mn, newdata = vars_test, type = "prob")
predclass <- apply(preds, 1, classify)
tbl <- table(predclass, vars_test$class)
score(tbl)
tbl
test_preds <- predict(mn, newdata = tests, type = "prob")
test_preds <- apply(test_preds, 1, classify)
csv_file <- data.frame("id" = tests$id,
                       "class" = test_preds)
write.csv(csv_file, "modelpredictions7_REFINED_LOG.csv", row.names = FALSE)
```


